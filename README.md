# Cause: Categorical User Sequence Compression

## Data Preprocessing

Main entry: `processor.py`  
Core code: `processors/*.py`

Based on the `BackboneProcessor` class, each dataset needs a custom `Processor`. You can refer to `KuairandProcessor`.

```bash
python processor.py --config config/processor/kuairand/recall.yaml --train 16 --test 23 --test_num 5 --val_num 5 --length 512 --n_core 5
```

The hyperparameters are defined in config/processor/kuairand/recall.yaml. They are ultimately passed via the config argument into BackboneProcessor and then unpacked inside KuairandProcessor.

You can add an alias for the current configuration via --name <alias>. If no alias is provided, you’ll need to remember the Signature printed during preprocessing, for example:

```text
[00:00:00] |KuairandProcessor| Dataset: kuairand
[00:00:00] |KuairandProcessor| Config: {'train_from': 16, 'test_at': 23, 'max_length': 512, 'min_length': 10, 'test_num': 5, 'val_num': 5, 'n_core': 5}
[00:00:00] |KuairandProcessor| Force: True
[00:00:00] |KuairandProcessor| Following configuration is signatured by @4qfRRUm8, also known as #recall_l512_c5:
[00:00:00] |KuairandProcessor| {
  "train_from": 16,
  "test_at": 23,
  "max_length": 512,
  "min_length": 10,
  "test_num": 5,
  "val_num": 5,
  "n_core": 5
}
```

If an alias has already been mapped to a specific configuration, an error will be raised by default. You can override the mapping with `--force true`.

With the current version, there’s no need to configure files under `config/data/*.yaml`.

For alternative action-scoring schemes or sequence-filtering strategies, please implement an additional Processor. It can inherit from `KuairandProcessor`, but must inherit from `BackboneProcessor`.

## Model Training

Main entry: `trainer.py`
```bash
python trainer.py --data kuairand.recall_l512_c5 --seq flatten --bz 16 --neg 200 --aw 0
```

In the current version, the `--data` argument only needs the Processor name plus the preprocessing Signature or alias. The following are equivalent:

```bash
--data kuairand.recall_l512_c5
--data kuairand.#recall_l512_c5  // Force using alias with the # symbol
--data kuairand.4qfRRUm8
--data kuairand.@4qfRRUm8  // Force using signature with the @ symbol
```

Supported hyperparameters for negative sampling:

| Argument  | Example                 | Description                                                                                                                           |
|-----------|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------|
| –data     | kuairand.recall_l512_c5 | For each dataset generated by a Processor, provide a YAML entry                                                                       |
| –model    | config/model/trm.yaml   | Currently supported backbones: transformer, hstu, and mamba                                                                           |
| –seq_type | flatten                 | Supported sequence types: HSTU’s flatten, Xiaohongshu’s stacked, and our io (input has item-only) (used in config/model/<model>.yaml) |
| –dim      | 64                      | Embedding dim, default 64 (used in config/exp/default.yaml)                                                                           |
| –bz       | 256                     | Batch size, default 256 (used in config/exp/default.yaml)                                                                             |
| –heads    | 8                       | Number of transformer/HSTU attention heads, default 8 (used in config/model/<model>.yaml)                                             |
| –layers   | 3                       | Number of layers, default 3 (used in config/model/<model>.yaml)                                                                       |
| –cuda     | 1                       | Usually no need to set manually; the least-loaded GPU is auto-selected                                                                |
| –load     | null                    | For training from scratch leave as null; for incremental training you can load a pre-trained model (used in config/exp/default.yaml)  |
| –lr       | 0.001                   | Default 0.001 (used in config/exp/default.yaml)                                                                                       |

